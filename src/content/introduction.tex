\chapter{Introduction}  
\label{introduction}
% Teil #1 – Worum geht es?                       = Fragestellung 
%                                                = 1. Die Erklärung der Problemstellung 
%                                                = die Fragestellung und generelle Zielsetzung deiner Abschlussarbeit
%                                                = Problemstatement
Neural Cellular Automatas (NCAs) are lightweight neural models and, therefore, highly suitable for medical image segmentation in low-resource environments, where the prevailing large-scale segmentation models, primarily based on U-Nets, cannot be used, as they require an expensive infrastructure \cite{kalkhof:2023:M3D-NCA}. 
Medical image analysis is aiming for high-impact applications, like faster diagnosis and computer-assisted surgery \cite{Isensee:2021:nnU-Net, Maier-Hain:2018:BioMedAnalysisOverview/Ranking} and Medical image segmentation is seen as a key technology for this \cite{Maier-Hain:2018:BioMedAnalysisOverview/Ranking}. Despite the emergence of the first certified software applications for clinical use, integrating machine learning models into clinical practice faces challenges. One hurdle for many resource-limited scenarios is the size of the current models. Most current state-of-the-art models, such as the nnU-Net, can only be used with a correspondingly expensive infrastructure \cite{Varaquaux:2022:medMLFailuresFuture, kalkhof:2023:M3D-NCA}. NCAs take a consequently different approach to the currently prevailing U-Net like models, that use a deep hierarchical architecture and process the information of the input as a whole. NCAs, on the other hand, only work on local information that is processed iteratively with the same function. Due to this architecture, they require much less space. NCAs can be deployed with very few resources and are, therefore, a promising approach for resource-constrained scenarios, like low-income regions, primary care facilities, and in crisis. As \cite{kalkhof:2023:medNCA} has shown, an NCA with 500 times fewer parameters and a 2\% to 10\% performance gap compared to the state-of-the-art nnU-Net can be deployed on a Raspberry Pi Model B+(US\$ 35).\\
Another big challenge machine learning for medical imaging and in general is facing currently is the robustness of models across different domains and, due to perturbations in data samples \cite{Yan:2019:DomainShiftsInMedSeg, Zhou:2023:DomainGeneralization_alsoAugmentation}. 
Robustness is the ability of models to make stable, i.e., correct, predictions with unknown data. This unknown data can be caused by external disturbances in the application or from a previously unseen domain \cite{Zhou:2023:DomainGeneralization_alsoAugmentation}. For example, from noise during image generation or due to a new MRI machine, respectively.
Adapting knowledge to new data is straightforward for humans. Humans apply what they have learned to new data all the time, even if it comes from other domains or has new and unknown perturbations. For example, humans can segment an object even in a noisy image or if the image has been created differently. Of course, the human adjustment capabilities go much further. Machine learning models, on the other hand, generalize very poorly \cite{Zhou:2023:DomainGeneralization_alsoAugmentation}. This also holds for NCAs and is of crucial importance for the medical sector, as people's health is at stake.\\
% !!! Das hier ist eigentlich schon Teil 2 ... oder noch nicht soo ganz
Unlike most standard models, like U-Nets, NCAs are stochastic in their output and still produce stable and high-quality segmentations. This means that the same NCA model on the same input produces (slightly) different outputs each time, but they all represent an accurate, valid segmentation.
\cite{kalkhof:2023:M3D-NCA} has developed a variance-based quality metric, the NCA Quality Score (NQM), based on this stochasticity, in order to make a comparable, scalar quality statement about individual NCA models.

In this work, we investigate whether this NCA Quality Score can also be used in the training cycle to improve the robustness of the model concerning domain shifts and data noise. 

% Teil #2 – Wie hast du das Thema bearbeitet?    = Methoden      
%                                                = 2. Inwiefern der Ansatz anders ist zu bestehenden und was die Intuition ist 
%                                                = Welche Methodik wurde wie angewendet? 
%                                                = Hypothesen und verwendete Methoden
The NQM is a scalar metric based on the variance across the outputs of an NCA. \cite{kalkhof:2023:M3D-NCA} used it to indicate the stability of the model and detect failure cases. Our approach utilizes the variance captured in the NQM metric for robustness improvement during training by including it in the loss function. Since, to the best of our knowledge, this has not been done before, we chose an exploratory approach, which can subsequently be divided into three steps. First, we adapted the NQM to work as a loss function (\autoref{experiments:01.0:Into}). Secondly, we developed several loss functions that use this adapted NQM and tested them against the standard loss of the models we use, the DiceBCE (\autoref{methods:NCA}). Here, the additive and linear weighted variant stood out, which we then, thirdly, used primarily to carry out our robustness tests (\autoref{experiments:03.0:Intro}).
For the robustness tests, we simulated radiological noise and domain shifts. To simulate radiological noise, we artificially augmented datasets with spike and noise artifacts that can occur during imaging \cite{Yan:2019:DomainShiftsInMedSeg, Zhou:2023:DomainGeneralization_alsoAugmentation}. We used \cite{torchIO} for this. To simulate domain shifts, where data of supposedly the same type comes from different subdomains,
we first used an existing medical dataset of an organ as the basic dataset. We then inserted individual or multiple samples from other datasets of the same organ into this dataset, and we created a combined dataset, which was put together from several datasets of the same organ. We have created seven series of datasets that way, four with augmentations and 3 with domain shifts. In total, we created 27 different datasets, 20 with augmentations and 7 with domain shifts. On each series, we trained at least one cohort of models with our loss function and one with the standard loss of the model. We tested them, at least on all datasets of the same series and compared the results.
Here, we used two different NCAs and two different medical datasets. The Backbone-NCA from \cite{kalkhof:2023:medNCA}, a basic, yet competent segmentation NCA, and the hippocampus dataset from \cite{Antonelli:2022:MedSegmentationDecatlon} with augmentations, has been used for the first explorations. The Med-NCA \cite{kalkhof:2023:medNCA}, a stronger one built from two Backbone-NCAs, has been used to explore transferability to another model. The prostate dataset from \cite{Antonelli:2022:MedSegmentationDecatlon} has been used with augmentations to test transferability to another dataset, and together with other prostate datasets, for domain shifts.
Furthermore, we tested whether the use of pretrained models can accelerate our approach, whether the number of outputs over which the variance is formed has an influence, whether the quality can be influenced with an additional hyperparameter, and whether nonlinear weightings of the NQM in the loss function can also work out.

% Teil #3 – Was sind die wichtigsten Ergebnisse? = Ergebnisse    
%                                                = 3. Was genau damit erreicht wurde, insgesamt, mit Präsentation der Ergebnisse 
%                                                = Was sind die wichtigsten Ergebnisse?
That way we developed a first working approach, improved it, and opened ways to improve further. Our additive and linear weighted variant of our NQM losses improves the robustness of the Backbone-NCA against radiological perturbations by up to 15 points on the Dice for augmented hippocampus datasets (\autoref{experiments:03.1.1:backbone_hippo:spike_noise}). Since the use of the NQM takes more computation time because multiple outputs have to be generated each time, we investigated whether using models pretrained on the loss without the NQM leads to equivalent results (\autoref{experiments:03.1.2:backbone_hippo:pretrained}). That way, we could speed up our approach a lot.
By showing that reducing the number of outputs for the variance to a minimum of two also leads to equivalent robustness (\autoref{experiments:03.1.3:backbone_hippo:stackSize}), the approach can be accelerated even further. On top of that, we could identify suitable candidates for further speed up, like a square-root variant (\autoref{experiments:03.1.7:backbone_hippo:sqrtNQM}) and when doubling the NQM in the loss (\autoref{experiments:03.1.4:backbone_hippo:alpha}).\\
Transferring this approach to the Med-NCA and the prostate dataset was not trivial, as massive scattering occurred initially in the tests for augmentations and domain shifts with single or few out-of-domain samples. For augmentations from -7 to +14 (\autoref{experiments:03.2.1:med_prost:augmented}), for domains shifts from -16 to +7 (\autoref{experiments:03.2.2:med_prost:onDomainShifts}). This could be resolved for augmentations with a pretrained model and an increased overall training time. That way, the approach stabilized. These adaptions led to an increase of up to +5 on the Dice in one test and overall to equivalent results as without the NQM. The initial scatter vanished. Initially, the scatter occurred only with models that were not fully converged. When using fully converged, pretrained models, these no longer occurred. Therefore, we recommend to use our approach with pretrained models that are fully converged.
Transferring the first results to the Med-NCA, by still using the hippocampus dataset was no problem on the other hand, although we also used a pretrained model here (\autoref{experiments:03.3.0:med_hippo:intro_and_Augmented}). In this setting, using the NQM loss also made no difference. Taking this together, it is reasonable to assume that the Med-NCA, due to its architecture, already leads to equivalent robustness, at least in this setting.
Additionally, for domain shifts on the merged dataset, our approach improved the robustness of the Med-NCA by up to +7 on Dice on one source dataset without deteriorating on others (\autoref{experiments:03.2.2:med_prost:onDomainShifts}).


Overall, with the NQM, an NCA can become much more robust to perturbations (up to +15 on Dice), and since we were able to eliminate most negative effects we faced,  we are confident that the ones on domain shifts can also be worked out the same way. However, this approach brings fewer advantages for Med-NCA than for Backbone-NCA. Due to the speedup, we were able to achieve and because there do not seem to be any disadvantages when using a fully converged model, nevertheless, it is attractive to perform a post-training on the NQM and validate the robustness gain. Especially for target datasets, we did not test in this thesis and with regard to larger combined datasets. Since our approach can easily be integrated into existing training pipelines.


The author's task for this thesis was to investigate whether the NQM, a variance-based quality metric introduced in \cite{kalkhof:2023:M3D-NCA}, can be used to improve the robustness of NCAs for medical image segmentation.\\
Therefore, the \textbf{related work} in the fields of NCAs, medical image segmentation, and robustness improvement will first be outlined in \autoref{Related Work}. Subsequently, the \textbf{methodology} background and some preliminary considerations that led to the implementation decision to extend the loss function by the NQM are presented in \autoref{methods:intro}. Followed by the wide range of exploratory \textbf{experiments} in \autoref{experiments:intro}, some discussions about open threads and \textbf{future work} in \autoref{future_work} and a culmination over the results and the \textbf{conclusions} in \autoref{conclusions}.